{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Read the CSV file with MultiIndex\n",
    "data_dir = f\"/isilon/datalake/cialab/original/cialab/image_database/d00154/Tumor_gene_counts/training_data_6_tumors.csv\"\n",
    "df = pd.read_csv(data_dir, header=[0, 1], index_col=0)\n",
    "df.columns = pd.MultiIndex.from_tuples(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_names = df.index.values\n",
    "gene_name_number_mapping = {gene_names[i]: i for i in range(len(gene_names))}\n",
    "gene_number_name_mapping = {i: gene_names[i] for i in range(len(gene_names))}\n",
    "gene_numbers = np.arange(len(gene_names))\n",
    "\n",
    "# 2. Reshape and Preprocess the Data\n",
    "# Flatten the DataFrame\n",
    "data = []\n",
    "for col in df.columns:\n",
    "    label = col[0]  # First level of the MultiIndex is the class name\n",
    "    features = df[col].values\n",
    "    data.append((features, label))\n",
    "\n",
    "# Separate features and labels\n",
    "features, labels = zip(*data)\n",
    "\n",
    "# Create a set of unique labels and sort it to maintain consistency\n",
    "unique_labels = sorted(set(labels))\n",
    "\n",
    "# Create a mapping dictionary from label to number\n",
    "label_to_number = {label: num for num, label in enumerate(unique_labels)}\n",
    "\n",
    "# Map your labels to numbers\n",
    "numerical_labels = [label_to_number[label] for label in labels]\n",
    "\n",
    "# To get the reverse mapping (from number to label), you can use:\n",
    "number_to_label = {num: label for label, num in label_to_number.items()}\n",
    "\n",
    "labels = numerical_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_numbers_len = len(gene_numbers)\n",
    "gene_numbers_len = np.round(np.sqrt(gene_numbers_len)) + 1\n",
    "print(gene_numbers_len)\n",
    "gene_num_2d = np.zeros((len(gene_numbers), 2))\n",
    "for i in range(len(gene_numbers)):\n",
    "    gene_num_2d[i, 0] = i // gene_numbers_len\n",
    "    gene_num_2d[i, 1] = i % gene_numbers_len\n",
    "print(gene_num_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_mean = np.mean(features)\n",
    "features_std = np.std(features)\n",
    "features_normalized = (features - features_mean) / features_std\n",
    "\n",
    "gene_numbers_mean = np.mean(gene_numbers)\n",
    "gene_numbers_std = np.std(gene_numbers)\n",
    "gene_numbers_normalized = (gene_numbers - gene_numbers_mean) / gene_numbers_std \n",
    "\n",
    "gene_num_2d_mean = np.mean(gene_num_2d)\n",
    "gene_num_2d_std = np.std(gene_num_2d)\n",
    "gene_num_2d_normalized = (gene_num_2d - gene_num_2d_mean) / gene_num_2d_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a Custom Dataset\n",
    "class TumorDataset(Dataset):\n",
    "    def __init__(self, features_count, features_gene_idx, labels):\n",
    "        self.features_count = features_count\n",
    "        self.features_idx = features_gene_idx\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample_feature1 = self.features_count[idx]\n",
    "        sample_feature2 = self.features_idx[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample_feature1, sample_feature2, label\n",
    "\n",
    "# 4. Split Dataset\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(features_normalized, labels, test_size=0.3, random_state=42, stratify=labels)  # feature normalization\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Ensuring the test set has equal number of samples for each class\n",
    "class_counts = Counter(y_test)\n",
    "min_class_count = min(class_counts.values())\n",
    "indices = {label: np.where(y_test == label)[0][:min_class_count] for label in class_counts}\n",
    "balanced_indices = np.concatenate(list(indices.values()))\n",
    "X_test_balanced = [X_test[i] for i in balanced_indices]\n",
    "y_test_balanced = [y_test[i] for i in balanced_indices]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_numbers_norm_tile_train = np.tile(gene_numbers_normalized, (X_train.shape[0], 1))\n",
    "gene_numbers_norm_tile_val = np.tile(gene_numbers_normalized, (X_val.shape[0], 1))\n",
    "gene_numbers_norm_tile_test = np.tile(gene_numbers_normalized, (len(X_test_balanced), 1))\n",
    "\n",
    "gene_num_2d_norm_tile_train = np.tile(gene_num_2d_normalized, (X_train.shape[0], 1, 1))\n",
    "gene_num_2d_norm_tile_val = np.tile(gene_num_2d_normalized, (X_val.shape[0], 1, 1))\n",
    "gene_num_2d_norm_tile_test = np.tile(gene_num_2d_normalized, (len(X_test_balanced), 1, 1))\n",
    "# Create PyTorch Datasets\n",
    "train_dataset = TumorDataset(X_train, gene_num_2d_norm_tile_train, y_train)\n",
    "val_dataset = TumorDataset(X_val, gene_num_2d_norm_tile_val, y_val)\n",
    "test_dataset = TumorDataset(X_test_balanced, gene_num_2d_norm_tile_test, y_test_balanced)\n",
    "\n",
    "# 5. Create DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data loader.\n",
    "for data_check in train_loader:\n",
    "    # Unpack the data\n",
    "    features1_check,features2_chekc, labels_check = data_check\n",
    "\n",
    "    # Print the first element of the batch\n",
    "    print(\"First feature batch:\", features1_check[0])\n",
    "    print(\"First feature batch:\", features2_chekc[0])\n",
    "    print(\"First label batch:\", labels_check[0])\n",
    "\n",
    "    # Break the loop after the first batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features1_check.shape, features2_chekc.shape, labels_check.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GSNet(nn.Module):\n",
    "    def __init__(self, k=2, out_k=3) -> None:\n",
    "        super(GSNet, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(k, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 64, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(64, out_k, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.out_k = out_k\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "class SNet(nn.Module):\n",
    "    def __init__(self, k=3):\n",
    "        super(SNet, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(k, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, k*k)\n",
    "        self.fc4 = nn.Linear(k*k, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "        self.bn6 = nn.BatchNorm1d(k*k)\n",
    "        self.k = k\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = F.relu(self.bn6(self.fc3(x)))\n",
    "        x = self.fc4(x)\n",
    "        x = F.pad(x, (0, self.k*self.k-1), 'constant', 0)\n",
    "\n",
    "        iden = np.eye(self.k)\n",
    "        # Set the first element to 0\n",
    "        iden[0, 0] = 0\n",
    "        iden_tensor = Variable(torch.from_numpy(iden.flatten().astype(np.float32))).view(1,self.k*self.k).repeat(batchsize,1)\n",
    "        if x.is_cuda:\n",
    "            iden_tensor = iden_tensor.cuda()\n",
    "        x = x + iden_tensor\n",
    "        x = x.view(-1, self.k, self.k)\n",
    "        return x\n",
    "\n",
    "\n",
    "class STNkd(nn.Module):\n",
    "    def __init__(self, k=64):\n",
    "        super(STNkd, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(k, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, k*k)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.bn4 = nn.BatchNorm1d(512)\n",
    "        self.bn5 = nn.BatchNorm1d(256)\n",
    "\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = torch.max(x, 2, keepdim=True)[0]\n",
    "        x = x.view(-1, 1024)\n",
    "\n",
    "        x = F.relu(self.bn4(self.fc1(x)))\n",
    "        x = F.relu(self.bn5(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        iden = Variable(torch.from_numpy(np.eye(self.k).flatten().astype(np.float32))).view(1,self.k*self.k).repeat(batchsize,1)\n",
    "        if x.is_cuda:\n",
    "            iden = iden.cuda()\n",
    "        x = x + iden\n",
    "        x = x.view(-1, self.k, self.k)\n",
    "        return x\n",
    "\n",
    "class attmil(nn.Module):\n",
    "\n",
    "    def __init__(self, inputd=1024, hd1=512, hd2=256):\n",
    "        super(attmil, self).__init__()\n",
    "\n",
    "        self.hd1 = hd1\n",
    "        self.hd2 = hd2\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            torch.nn.Conv1d(inputd, hd1, 1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.attention_V = nn.Sequential(\n",
    "            torch.nn.Conv1d(hd1, hd2,1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.attention_U = nn.Sequential(\n",
    "            torch.nn.Conv1d(hd1, hd2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.attention_weights = torch.nn.Conv1d(hd2, 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x) # b*512*n\n",
    "\n",
    "        A_V = self.attention_V(x)  # b*256*n\n",
    "        A_U = self.attention_U(x)  # b*256*n\n",
    "        A = self.attention_weights(A_V * A_U) # element wise multiplication # b*1*n\n",
    "        A = A.permute(0, 2, 1)  # b*n*1\n",
    "        A = F.softmax(A, dim=1)  # softmax over n\n",
    "\n",
    "        # M = torch.matmul(A, x)  # 1x512\n",
    "        # M = M.view(-1, self.hd1) # 512\n",
    "\n",
    "        # Y_prob = self.classifier(M)\n",
    "\n",
    "        # return Y_prob, A\n",
    "        return A # batch_size x 1 x n\n",
    "    \n",
    "class PointNetfeat(nn.Module):\n",
    "    def __init__(self, input_dim = 4, fstn_dim = 64, global_feat = True, feature_transform = False, atention_pooling_flag = False):\n",
    "        super(PointNetfeat, self).__init__()\n",
    "        self.stn = STNkd(k=input_dim)\n",
    "        self.conv1 = torch.nn.Conv1d(input_dim, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.bn3 = nn.BatchNorm1d(1024)\n",
    "        self.global_feat = global_feat\n",
    "        self.feature_transform = feature_transform\n",
    "        if self.feature_transform:\n",
    "            self.fstn = STNkd(k=fstn_dim)\n",
    "        if atention_pooling_flag:\n",
    "            self.atention_pooling = attmil()\n",
    "        self.atention_pooling_flag = atention_pooling_flag\n",
    "\n",
    "    def forward(self, x):\n",
    "        n_pts = x.size()[2]\n",
    "        trans = self.stn(x)\n",
    "        x = x.transpose(2, 1)\n",
    "        x = torch.bmm(x, trans)\n",
    "        x = x.transpose(2, 1)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "\n",
    "        if self.feature_transform:\n",
    "            trans_feat = self.fstn(x)\n",
    "            x = x.transpose(2,1)\n",
    "            x = torch.bmm(x, trans_feat)\n",
    "            x = x.transpose(2,1)\n",
    "        else:\n",
    "            trans_feat = None\n",
    "\n",
    "        pointfeat = x\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.bn3(self.conv3(x))\n",
    "\n",
    "\n",
    "        if self.atention_pooling_flag:\n",
    "            A = self.atention_pooling(x)\n",
    "            x = torch.bmm(x, A)\n",
    "        else:\n",
    "            x = torch.max(x, 2, keepdim=True)[0] ######## think about how to change it to attention pooling\n",
    "        x = x.view(-1, 1024)\n",
    "        if self.global_feat:\n",
    "            return x, trans, trans_feat\n",
    "        else:\n",
    "            x = x.view(-1, 1024, 1).repeat(1, 1, n_pts)\n",
    "            return torch.cat([x, pointfeat], 1), trans, trans_feat\n",
    "\n",
    "class PointNetCls(nn.Module):\n",
    "    def __init__(self, gene_idx_dim = 2, gene_space_num = 3, class_num=10, feature_transform=False, atention_pooling_flag = False):\n",
    "        super(PointNetCls, self).__init__()\n",
    "        self.gstn = GSNet(k=gene_idx_dim)\n",
    "        self.feature_transform = feature_transform\n",
    "        self.feat = PointNetfeat(input_dim = gene_space_num+1, global_feat=True, feature_transform=feature_transform, atention_pooling_flag = atention_pooling_flag)\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, class_num)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x_feature, x_gene_idx):\n",
    "        x_gene_idx = self.gstn(x_gene_idx)\n",
    "        x = torch.cat([x_feature, x_gene_idx], 1)\n",
    "        x, trans, trans_feat = self.feat(x)\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = F.relu(self.bn2(self.dropout(self.fc2(x))))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1), trans, trans_feat\n",
    "\n",
    "\n",
    "class PointNetDenseCls(nn.Module):\n",
    "    def __init__(self, k = 2, feature_transform=False):\n",
    "        super(PointNetDenseCls, self).__init__()\n",
    "        self.k = k\n",
    "        self.feature_transform=feature_transform\n",
    "        self.feat = PointNetfeat(global_feat=False, feature_transform=feature_transform)\n",
    "        self.conv1 = torch.nn.Conv1d(1088, 512, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(512, 256, 1)\n",
    "        self.conv3 = torch.nn.Conv1d(256, 128, 1)\n",
    "        self.conv4 = torch.nn.Conv1d(128, self.k, 1)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batchsize = x.size()[0]\n",
    "        n_pts = x.size()[2]\n",
    "        x, trans, trans_feat = self.feat(x)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.conv4(x)\n",
    "        x = x.transpose(2,1).contiguous()\n",
    "        x = F.log_softmax(x.view(-1,self.k), dim=-1)\n",
    "        x = x.view(batchsize, n_pts, self.k)\n",
    "        return x, trans, trans_feat\n",
    "\n",
    "    \n",
    "def feature_transform_regularizer(trans):\n",
    "    d = trans.size()[1]\n",
    "    batchsize = trans.size()[0]\n",
    "    I = torch.eye(d)[None, :, :]\n",
    "    if trans.is_cuda:\n",
    "        I = I.cuda()\n",
    "    loss = torch.mean(torch.norm(torch.bmm(trans, trans.transpose(2,1)) - I, dim=(1,2)))\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsnet torch.Size([8, 3, 60660])\n"
     ]
    }
   ],
   "source": [
    "# test GSNet\n",
    "sim_data_gene_idx = Variable(torch.rand(8, 60660, 2))\n",
    "sim_data_gene_idx = sim_data_gene_idx.transpose(2, 1)\n",
    "gstn = GSNet(k=2, out_k=3)\n",
    "gene_space = gstn(sim_data_gene_idx)\n",
    "print('gsnet', gene_space.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x torch.Size([8, 4, 60660])\n"
     ]
    }
   ],
   "source": [
    "# test Concat\n",
    "sim_data_feature = Variable(torch.rand(8, 60660))\n",
    "sim_data_feature = sim_data_feature.unsqueeze(1)\n",
    "x = torch.cat([sim_data_feature, gene_space], 1)\n",
    "print('x', x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test fstn\n",
    "trans = STNkd(k=4)\n",
    "out = trans(x)\n",
    "print('stn', out.size())\n",
    "print('loss', feature_transform_regularizer(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global feat torch.Size([8, 1024])\n"
     ]
    }
   ],
   "source": [
    "# test PointNetfeat (max pooling)\n",
    "pointfeat = PointNetfeat(input_dim = 4, fstn_dim = 64, global_feat = True, feature_transform = False, atention_pooling_flag = False)\n",
    "pointfeat.to(device)\n",
    "x = x.to(device)\n",
    "out, _, _ = pointfeat(x)\n",
    "\n",
    "print('global feat', out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test attention pooling\n",
    "att_pl =attmil(inputd=1024, hd1=512, hd2=256)\n",
    "sim_input = Variable(torch.rand(32, 1024, 60660))\n",
    "A = att_pl(sim_input)\n",
    "print('attention pooling', A.size())\n",
    "sum = torch.sum(A, dim=1)\n",
    "print('sum', sum.size())\n",
    "print('sum', sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_output = torch.bmm(sim_input,A)\n",
    "print('sim_output', sim_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global feat torch.Size([8, 1024])\n"
     ]
    }
   ],
   "source": [
    "# test PointNetfeat (attention pooling)\n",
    "device = torch.device(\"cuda:0\")\n",
    "pointfeat = PointNetfeat(input_dim = 4, fstn_dim = 64, global_feat = True, feature_transform = False, atention_pooling_flag = True)\n",
    "pointfeat.to(device)\n",
    "x = x.to(device)\n",
    "out, _, _ = pointfeat(x)\n",
    "print('global feat', out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls_pointnet torch.Size([2, 10])\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# test PointNetCls\n",
    "x_feature, x_gene_idx = Variable(torch.rand(2, 60660)), Variable(torch.rand(2, 60660, 2))\n",
    "x_gene_idx = x_gene_idx.transpose(2, 1)\n",
    "x_feature = x_feature.unsqueeze(1)\n",
    "cls_pointnet = PointNetCls(gene_idx_dim = 2, gene_space_num = 3, class_num=10, feature_transform=False, atention_pooling_flag = False)\n",
    "cls_pointnet.to(device)\n",
    "x_feature = x_feature.to(device)\n",
    "x_gene_idx = x_gene_idx.to(device)\n",
    "out, _, _ = cls_pointnet(x_feature, x_gene_idx)\n",
    "print('cls_pointnet', out.size())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
